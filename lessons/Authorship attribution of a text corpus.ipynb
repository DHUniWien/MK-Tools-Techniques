{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorship attribution of a text corpus\n",
    "=======================================\n",
    "\n",
    "Here we will repeat a famous experiment in authorship attribution, and try to discover who wrote the Federalist Papers!\n",
    "\n",
    "We have the corpus from our lesson on NLTK, and we have the `gensim` library that we used in our topic modeling experiments. We'll put these together to get what we need for authorship attribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the data\n",
    "-------------------\n",
    "\n",
    "Now let's load up the Papers. They are in a folder called 'federalist' and each paper is numbered, e.g. 'federalist_7.txt'. We can just as easily do this using NLTK to make a corpus out of the folder, as we did last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus.reader.util import read_regexp_block\n",
    "\n",
    "# Define how paragraphs look in our text files.\n",
    "def read_hanging_block( stream ):\n",
    "    return read_regexp_block( stream, \"^[A-Za-z]\" )\n",
    "\n",
    "corpus_root = '../lessondata/federalist'\n",
    "file_pattern = 'federalist_.*\\.txt'\n",
    "federalist = PlaintextCorpusReader( corpus_root, file_pattern, \n",
    "                                para_block_reader=read_hanging_block )\n",
    "print(\"List of texts in corpus:\", federalist.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorship attribution is done by comparing different *features* of the texts we are looking at. Examples include:\n",
    "\n",
    "* lexical features (average sentence length, variation in sentence length, range of words used)\n",
    "* punctuation features (average number of different marks per sentence)\n",
    "* word count features (e.g. frequency of the different common 'function words')\n",
    "* syntactic features (e.g. frequency of noun use, verb use, adjective use, etc.)\n",
    "\n",
    "Essentially there are a whole lot of approaches to take, and usually you want to take as many approaches as possible to arrive at some sort of consensus answer. Today we will try three approaches: looking at use of function words, at lexical diversity, and at relative frequency of parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the word count feature - the frequency of \"function words\"\n",
    "------------------------------------------------------------------\n",
    "\n",
    "These are the words that we would normally leave out of any vocabulary analysis because they are so common - 'the', 'a', 'and', 'of', 'to', and so on. Indeed we left them out of our topic modeling trial last week for this very reason, but for authorship attribution, conversely, they might be very relevant! Let's retrieve them from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(\" : \".join(stopwords.words(\"english\")))\n",
    "print(len(stopwords.words(\"english\")))\n",
    "\n",
    "# Make the stopword list into a Python set. That will make our work much faster below.\n",
    "swset = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! Now we have, for each text, to count up the frequency of each of these words. This is called making a \"feature vector\" - each text will be reduced to a data structure that has a count for each of the function words.\n",
    "\n",
    "**PAY ATTENTION HERE!** This step, the conversion of text files to feature vectors, is where you will make or break any of these text analysis techniques. As we will see, when we are doing authorship attribution we want to count the stopwords, but when we do topic modeling we want to count everything *BUT* the stopwords! Think carefully about the theory and ideas behind what you are doing, when you use these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "# Make a dictionary for each text that gives the frequency of each stopword\n",
    "stopword_vectors = []\n",
    "for paper in federalist.fileids():\n",
    "    # Get all the words in the paper as lowercase\n",
    "    pwords = [x.lower() for x in federalist.words(paper)]\n",
    "    # Generate a vocabulary from these words\n",
    "    pvocab = Text(pwords).vocab()\n",
    "    # Keep the stopword entries in the vocabulary\n",
    "    pstopword_freq = {k: pvocab[k] for k in pvocab.keys() if k in swset}\n",
    "    # Add in all the stopwords that are *not* in the vocabulary\n",
    "    for sw in swset:\n",
    "        if pvocab.get(sw) is None:\n",
    "            pstopword_freq[sw] = 0\n",
    "    # Now save this dictionary in our list.\n",
    "    stopword_vectors.append(pstopword_freq)\n",
    "    \n",
    "stopword_vectors[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something similar to get the distribution of parts of speech. We will POS-tag all the texts, choose the twenty most common parts of speech throughout the corpus excluding punctuation, and then make a similar vector for each text counting the instances of each part of speech.\n",
    "\n",
    "Here is how to tag a single text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tag(federalist.words('federalist_1.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...so let's do this for all the texts, and put the resulting arrays into an outer array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert sequences of words into sequences of part-of-speech tags\n",
    "pos_texts = []\n",
    "for fed in federalist.fileids():\n",
    "    pos_tagged = pos_tag(federalist.words(fed))\n",
    "    pos_texts.append(pos_tagged)\n",
    "print(len(pos_texts))\n",
    "pos_texts[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Figure out what our top 15 parts of speech are by making a single long \"sequence\" and getting its vocabulary\n",
    "pos_corpus = []\n",
    "for pt in pos_texts:\n",
    "    pos_corpus.extend([x[1] for x in pt if x[1].isalpha()])\n",
    "len(pos_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_frequent_pos = set([x[0] for x in Text(pos_corpus).vocab().most_common(15)])\n",
    "most_frequent_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now make the feature vector \n",
    "pos_vectors = []\n",
    "for pt in pos_texts:\n",
    "    tagsonly = [x[1] for x in pt]\n",
    "    pos_vocab = Text(tagsonly).vocab()\n",
    "    pos_freq = {k: pos_vocab[k] for k in pos_vocab.keys() if k in most_frequent_pos}\n",
    "    pos_vectors.append(pos_freq)\n",
    "\n",
    "print(len(pos_vectors))\n",
    "pos_vectors[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have extracted two sets of features our texts; one represents the frequency of function words, and the other represents the frequency of common parts of speech.\n",
    "\n",
    "But now we will want to normalize our vectors a little bit - some texts are a lot longer than others, so will have many more function words overall, and we don't want this fact to affect our results. So we need to scale the values in each dictionary, so that the word / POS count becomes a fraction of the text length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textlengths = [len(federalist.words(x)) for x in federalist.fileids()]\n",
    "\n",
    "def scale(vector, textlength):\n",
    "    # The vector is a dictionary of 'thing': 'count'. We need to scale the count\n",
    "    # in each case by the overall text length.\n",
    "    scaled = {}\n",
    "    for k in vector.keys():\n",
    "        scaled[k] = vector[k] / textlength\n",
    "    return scaled\n",
    "\n",
    "stopword_scaled = []\n",
    "pos_scaled = []\n",
    "for i in range(len(textlengths)):\n",
    "    stopword_scaled.append(scale(stopword_vectors[i], textlengths[i]))\n",
    "    pos_scaled.append(scale(pos_vectors[i], textlengths[i]))\n",
    "        \n",
    "print(len(pos_scaled))\n",
    "print(len(stopword_scaled))\n",
    "stopword_scaled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a dataframe\n",
    "----\n",
    "\n",
    "Now we need to put these dictionaries into a big table, called a 'dataframe'. This is done with a library called pandas, which is used a lot for all sorts of scientific computing in Python. If you are in the Digital Data class you can think of a dataframe as an SQL-like table. First let's try it with just the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = ['Paper #%s' % n.replace('federalist_', '').replace('.txt', '') for n in federalist.fileids()]\n",
    "\n",
    "stopword_features = pd.DataFrame(stopword_scaled, index=labels)\n",
    "pos_features = pd.DataFrame(pos_scaled, index=labels)\n",
    "stopword_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the results\n",
    "-------------------\n",
    "Okay! We have a set of criteria - the frequency of our function words - and a corresponding set of values for each text. It's time to crunch the numbers and see which papers resemble each other.\n",
    "\n",
    "We know that there were three authors, so we want to see if we can make the 85 different papers cluster into three groups. There is a statistical function for this called KMeans, from the \"scikit-learn\" module which has a lot of things for machine learning. (Dividing data into clusters of similar things is a pretty common thing to have to do in machine learning. Lucky for us.)\n",
    "\n",
    "First let's see what happens if we ask the computer to divide our papers into four groups (one for each author, plus one for the jointly-authored Hamilton/Madison ones.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def PredictAuthors(fvs):\n",
    "    km = KMeans(n_clusters=4)\n",
    "    km.fit(fvs)\n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we run this on our data table of the function word frequencies and get a complicated result. We ask for the labels of that result and get something that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopword_result = PredictAuthors( stopword_features ).labels_ \n",
    "print(stopword_result)\n",
    "pos_result = PredictAuthors( pos_features ).labels_\n",
    "print(pos_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these numbers (0, 1, 2, 3) represents an author. We know that Hamilton was responsible for most of the papers, Madison for most of the rest, Jay for a few, and the Hamilton/Madison collaboration for the fewest. So let's assign the authors on that assumption.\n",
    "\n",
    "(...How else might we model this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "author_order = [\"Hamilton\", \"Madison\", \"Jay\", \"Hamilton/Madison\"]\n",
    "\n",
    "freq_order = FreqDist(stopword_result).most_common(4)\n",
    "print(freq_order)\n",
    "\n",
    "mapping = {}\n",
    "for i in range(4):\n",
    "    mapping[freq_order[i][0]] = author_order[i]\n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put this into a function definition, since we'll have to do it twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assign_author(result):\n",
    "    author_order = [\"Hamilton\", \"Madison\", \"Jay\", \"Hamilton/Madison\"]\n",
    "    freq_order = FreqDist(result).most_common(4)\n",
    "    mapping = {}\n",
    "    for i in range(4):\n",
    "        mapping[freq_order[i][0]] = author_order[i]\n",
    "        \n",
    "    return [mapping.get(x) for x in result]\n",
    "\n",
    "assign_author(stopword_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how did that do against reality? Let's read in the real answers and add them to an HTML table for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../lessondata/federalist/metadata.txt', encoding='utf-8') as f:\n",
    "    answers = f.readlines()\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how this file looks: the first 3 characters have a number, then the next few (up to #30 - I counted for you - have the author(s), then the rest is the title of the paper. We can use this to make a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "known_authors = {}\n",
    "for a in answers:\n",
    "    paperno = int(a[0:3].lstrip())\n",
    "    author = a[4:29].rstrip()\n",
    "    known_authors[paperno] = author\n",
    "known_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that again, keeping the ones that are by a single author (e.g. JAY), discarding the ones that are by two authors (e.g. HAMILTON AND MADISON), and setting aside the ones that are uncertain (i.e. HAMILTON OR MADISON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disputed_papers = []\n",
    "known_authors = {}\n",
    "for a in answers:\n",
    "    paperno = int(a[0:3].lstrip())\n",
    "    author = a[4:29].rstrip()\n",
    "    if ' OR ' in author:\n",
    "        disputed_papers.append(paperno)\n",
    "    elif ' ' not in author:\n",
    "        known_authors[paperno] = author\n",
    "print(disputed_papers)\n",
    "known_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our \"real\" answers, we can make our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "stopword_authors = assign_author(stopword_result)\n",
    "pos_authors = assign_author(pos_result)\n",
    "\n",
    "def colorcode(assigned, real):\n",
    "    cellcolor = 'red'\n",
    "    if assigned.lower() == real.lower():\n",
    "        cellcolor = 'green'\n",
    "    elif real.lower().find(assigned.lower()) > -1:\n",
    "        cellcolor = 'orange'\n",
    "    return cellcolor\n",
    "\n",
    "answer_table = '<table><tr><th>Paper</th><th>Stopwords</th><th>Parts of speech</th><th>Real</th></tr>'\n",
    "for i in range(len(stopword_authors)):\n",
    "    if i in known_authors.keys():\n",
    "        ra = known_authors.get(i)\n",
    "    elif i in disputed_papers:\n",
    "        ra = 'UNKNOWN'\n",
    "    else:\n",
    "        ra = 'Hamilton and Madison'\n",
    "    sa = stopword_authors[i]\n",
    "    pa = pos_authors[i]\n",
    "    answer_table += '<tr><td>%d</td>' % (i+1)     # Print the letter number\n",
    "    answer_table += '<td style=\"color: %s;\">%s</td>' % (colorcode(sa, ra), sa)\n",
    "    answer_table += '<td style=\"color: %s;\">%s</td>' % (colorcode(pa, ra), pa)\n",
    "    answer_table += '<td>%s</td></tr>' % ra\n",
    "answer_table += '</table>'\n",
    "\n",
    "HTML(answer_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...Pretty terrible. ðŸ˜„ This is the difference between untrained and trained data. What happens if we tell the algorithm what we know, and then let it test the others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting our data\n",
    "----\n",
    "\n",
    "Now that we have made these feature vectors for all our texts, we need to set aside the ones whose authorship is unknown. Let's go through the `metadata.txt` file in the `federalist` directory, and see who wrote what."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting our dataframes\n",
    "----\n",
    "\n",
    "Let's remove those papers that are not in known_authors, to make the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopword_training = stopword_features.filter(items=[\"Paper #%d\" % x for x in known_authors.keys()], axis=0)\n",
    "pos_training = pos_features.filter(items=[\"Paper #%d\" % x for x in known_authors.keys()], axis=0)\n",
    "\n",
    "stopword_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopword_test = stopword_features.filter(items=[\"Paper #%d\" % x for x in disputed_papers], axis=0)\n",
    "pos_test = pos_features.filter(items=[\"Paper #%d\" % x for x in disputed_papers], axis=0)\n",
    "pos_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervising the learning\n",
    "----\n",
    "\n",
    "We will use another form of clustering for this - rather than KMeans, it will be KNeighbors. [how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Get our list of answers, ordered by paper number.\n",
    "training_answers = [known_authors.get(p) for p in sorted(known_authors.keys())]\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(stopword_training.values, training_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = knn.predict(stopword_test.values)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn all this into a function, so we can try it with both sets of testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_predict(trainingset, answers, testset):\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(trainingset.values, answers)\n",
    "    return knn.predict(testset)\n",
    "\n",
    "cluster_predict(pos_training, training_answers, pos_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now what if we try to take into account both sets of data? We can combine the two training tables, and the two test tables, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_all = stopword_training.merge(pos_training, left_index=True, right_index=True)\n",
    "testing_all = stopword_test.merge(pos_test, left_index=True, right_index=True)\n",
    "\n",
    "cluster_predict(training_all, training_answers, testing_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost onto something!\n",
    "\n",
    "Probably the most commonly-used method for authorship attribution today is known as Burrows' Delta, named after John Burrows who came up with it. The Delta algorithms are available in [a statistical package](https://sites.google.com/site/computationalstylistics/) called `stylo`, written for the R programming language for statistical computing. If this is something you anticipate wanting to use, that is a very good place to start."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
