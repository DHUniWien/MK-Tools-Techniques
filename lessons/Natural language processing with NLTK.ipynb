{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to NLTK\n",
    "====================\n",
    "\n",
    "NLTK is the *Natural Language Toolkit*, a fairly large Python library for doing many sorts of linguistic analysis of text. NLTK comes with a selection of sample texts that we'll use today, to get yourself familiar with what sorts of analysis you can do.\n",
    "\n",
    "To run this notebook you will need the `nltk`, `matplotlib`, and `tkinter` modules, all of which come included with Anaconda. The `nltk` module needs a bit of initialization, which it is best to do from the command line.\n",
    "\n",
    "    python -c 'import nltk; nltk.download()'\n",
    "    \n",
    "and when an interactive dialog appears, use it to download the 'book' package.\n",
    "\n",
    "Examining features of a text\n",
    "--------------------------\n",
    "We will start by loading the example texts in the 'book' package that we just downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `import` statement reads the book samples, which include nine sentences and nine book-length texts. It has also helpfully put each of these texts into a variable for us, from `sent1` to `sent9` and `text1` to `text9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sent1)\n",
    "print(sent3)\n",
    "print(sent5)\n",
    "sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the texts now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(text6)\n",
    "print(text6.name)\n",
    "print(\"This text has %d words\" % len(text6.tokens))\n",
    "print(\"The first hundred words are:\", \" \".join( text6.tokens[:100] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these texts is an nltk.text.Text object, and has methods to let you see what the text contains. But you can also treat it as a plain old list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(text5[0])\n",
    "print(text3[0:11])\n",
    "print(text4[0:51])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do simple concordancing, printing the context for each use of a word throughout the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text6.concordance( \"swallow\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default is to show no more than 25 results for any given word, but we can change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text6.concordance('Arthur', lines=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adjust the amount of context we show in our concordance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text6.concordance('Arthur', width=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or get the number of times any individual word appears in the text. But **be careful** - while the concordance doesn't care about upper- or lowercase, the word count / word frequency logic does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_count = \"Knight\"\n",
    "print(\"The word %s appears %d times.\" % ( \n",
    "        word_to_count, text6.count( word_to_count ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a vocabulary for the text, and use the vocabulary to find the most frequent words as well as the ones that appear only once (a.k.a. the __hapaxes__.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t6_vocab = text6.vocab()\n",
    "t6_words = list(t6_vocab.keys())\n",
    "print(\"The text has %d different words\" % ( len( t6_words ) ))\n",
    "print(\"Some arbitrary 50 of these are:\", t6_words[:50])\n",
    "print(\"The most frequent 50 words are:\", t6_vocab.most_common(50))\n",
    "print(\"The word swallow appears %d times\" % ( t6_vocab['swallow'] ))\n",
    "print(\"The text has %d words that appear only once\" % ( \n",
    "        len( t6_vocab.hapaxes() ) ))\n",
    "print(\"Some arbitrary 100 of these are:\", t6_vocab.hapaxes()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now seen two methods for getting the number of times a word appears in a text: `t6.count(word)` and `t6_vocab[word]`. These are interchangeable - use whichever one you like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try and find interesting words in the text, such as words of a minimum length (the longer a word, the less common it probably is) that occur more than once or twice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a list of long words!\n",
    "# The short way, with a list comprehension\n",
    "long_words = [ w for w in t6_words if len( w ) > 5 and t6_vocab[w] > 3 ]\n",
    "\n",
    "# The long way, with a for loop. THIS IS IDENTICAL TO THE ABOVE.\n",
    "long_words = []\n",
    "for w in t6_words:\n",
    "    if( len ( w ) > 5 and t6_vocab[w] > 3 ):\n",
    "        long_words.append( w )\n",
    "\n",
    "# Now use the list!\n",
    "print(\"The reasonably frequent long words in the text are:\", long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can look for pairs of words that go together more often than chance would suggest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nUp to twenty collocations\")\n",
    "text6.collocations()\n",
    "\n",
    "print(\"\\nUp to fifty collocations\")\n",
    "text6.collocations(num=50)\n",
    "\n",
    "print(\"\\nCollocations that might have one word in between\")\n",
    "text6.collocations(window_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK can also provide us with a few simple graph visualizations, **when we have matplotlib installed**. To make this work in Jupyter/iPython, we need the following magic line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab --no-import-all inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary we get from the `.vocab()` method is something called a \"frequency distribution\", which means it's a giant tally of each unique word and the number of times that word appears in the text. We can also make a frequency distribution of other features, such as \"each possible word length vs. the number of times a word of that length is used\". Let's do that and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_length_dist = FreqDist( [ len(w) for w in t6_vocab.keys() ] )\n",
    "word_length_dist.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot where in the text a word occurs, and compare it to other words, with a *dispersion plot*. For example, the following dispersion plots show respectively (among other things) that the words 'coconut' and 'swallow' almost always appear in the same part of the *Holy Grail* text, and that Willoughby and Lucy do not appear in *Sense and Sensibility* until some time after the beginning of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text6.dispersion_plot([\"coconut\", \"swallow\", \"KNIGHT\", \"witch\", \n",
    "                       \"ARTHUR\"])\n",
    "\n",
    "text2.dispersion_plot([\"Elinor\", \"Marianne\", \"Edward\", \"Willoughby\", \n",
    "                       \"Lucy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go a little crazy with text statistics. This block of code computes the average word length for each text, as well as a measure known as the \"lexical diversity\" that measures how much word re-use there is in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_text_stats( thetext ):\n",
    "    # Average word length\n",
    "    awl = sum([len(w) for w in thetext]) / len( thetext ) \n",
    "    ld = len( thetext ) / len( thetext.vocab() )\n",
    "    print(\"%.2f\\t%.2f\\t%s\" % ( awl, ld, thetext.name ))\n",
    "    \n",
    "all_texts = [ text1, text2, text3, text4, text5, text6, text7, \n",
    "             text8, text9 ]\n",
    "print(\"Wlen\\tLdiv\\tTitle\")\n",
    "for t in all_texts:\n",
    "    print_text_stats( t )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text of your own\n",
    "------------------\n",
    "\n",
    "So far we have been using the sample texts, but we can also use any text that we have lying around on our computer. The easiest sort of text to read in is plaintext, not PDF or HTML or anything else. Once we have made the text into an NLTK text with the `Text()` function, we can use all the same methods on it as we did for the sample texts above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.text import Text\n",
    "\n",
    "# Read all the file's contents into Python.\n",
    "with open('../lessondata/alice.txt', encoding='utf-8') as f:\n",
    "    raw = f.read()\n",
    "\n",
    "# Use NLTK to break the text up into words, and put the result into a \n",
    "# Text object.\n",
    "alice = Text( word_tokenize( raw ) )\n",
    "print(alice.name)\n",
    "alice.name = \"Alice's Adventures in Wonderland\"\n",
    "print(alice.name)\n",
    "alice.concordance( \"cat\" )\n",
    "print_text_stats( alice )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using text corpora\n",
    "------------------\n",
    "\n",
    "NLTK comes with several pre-existing corpora of texts, some of which are the main body of text used for certain sorts of linguistic research. Using a corpus of texts, as opposed to an individual text, brings us a few more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "print(gutenberg.fileids())\n",
    "paradise_lost = Text( gutenberg.words( \"milton-paradise.txt\" ) )\n",
    "paradise_lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Paradise Lost* is now a Text object, just like the ones we have worked on before. But we accessed it through the *NLTK corpus reader*, which means that we get some extra bits of functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Length of text is:\", len( \n",
    "        gutenberg.raw( \"milton-paradise.txt\" )))\n",
    "print(\"Number of words is:\", len( \n",
    "        gutenberg.words( \"milton-paradise.txt\" )))\n",
    "assert( len( gutenberg.words( \"milton-paradise.txt\" )) \\\n",
    "       == len( paradise_lost ))\n",
    "print(\"Number of sentences is:\", len( \n",
    "        gutenberg.sents( \"milton-paradise.txt\" )))\n",
    "print(\"Number of paragraphs is:\", len( \n",
    "        gutenberg.paras( \"milton-paradise.txt\" )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make our own corpus if we have our own collection of files, e.g. the Federalist Papers. But we have to pay attention to how those files are arranged! In this case, if you look in the text file, the paragraphs are set apart with 'hanging indentation' - all the lines that are *not* the beginning of a paragraph begin with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus.reader.util import read_regexp_block\n",
    "\n",
    "# Tell NLTK how to know when a new paragraph starts in our text files.\n",
    "def read_hanging_block( stream ):\n",
    "    return read_regexp_block( stream, \"^[A-Za-z]\" )\n",
    "\n",
    "# Tell NLTK where our texts are.\n",
    "corpus_root = '../lessondata/federalist'\n",
    "# Tell NLTK how to know which files contain the texts for our corpus.\n",
    "file_pattern = 'federalist_.*\\.txt'\n",
    "federalist = PlaintextCorpusReader( corpus_root, file_pattern, \n",
    "                                para_block_reader=read_hanging_block )\n",
    "print(\"List of texts in corpus:\", federalist.fileids())\n",
    "print(\"\\nHere is the fourth paragraph of the first text:\")\n",
    "print(federalist.paras(\"federalist_1.txt\")[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like before, from this corpus we can make individual Text objects, on which we can use the methods we have seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fed1 = Text( federalist.words( \"federalist_1.txt\" ))\n",
    "print(\"The first Federalist Paper has the following word collocations:\")\n",
    "fed1.collocations()\n",
    "print(\"\\n...and the following most frequent words.\")\n",
    "fed1.vocab().most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out stopwords\n",
    "-----------------------\n",
    "\n",
    "In linguistics, *stopwords* or *function words* are words that are so frequent in a particular language that they say little to nothing about the meaning of a text. You can make your own list of stopwords, but NLTK also provides a list for each of several common languages. These sets of stopwords are provided as another corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(\"We have stopword lists for the following languages:\")\n",
    "print(stopwords.fileids())\n",
    "print(\"\\nThese are the NLTK-provided stopwords for the German language:\")\n",
    "print(\", \".join( stopwords.words('german') ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So reading in the stopword list, we can use it to filter out vocabulary we don't want to see. Let's look at our 50 most frequent words in *Holy Grail* again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The most frequent words are: \")\n",
    "print([word[0] for word in t6_vocab.most_common(50)])\n",
    "\n",
    "f1_most_frequent = [ w[0] for w in t6_vocab.most_common() \n",
    "                    if w[0].lower() not in stopwords.words('english') ]\n",
    "print(\"\\nThe most frequent interesting words are:  \", \n",
    "      \"  \".join( f1_most_frequent[:50] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we should get rid of punctuation and all-caps words too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_interesting( w ):\n",
    "    if( w.lower() in stopwords.words('english') ):\n",
    "        return False\n",
    "    if( w.isupper() ):\n",
    "        return False\n",
    "    return w.isalpha()\n",
    "\n",
    "f1_most_frequent = [ w[0] for w in t6_vocab.most_common() \n",
    "                    if is_interesting( w[0] ) ]\n",
    "print(\"The most frequent interesting words are: \", \n",
    "      \"  \".join( f1_most_frequent[:50] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of words that begins to make us think of *Monty Python and the Holy Grail*!\n",
    "\n",
    "Part-of-speech tagging\n",
    "----------------------\n",
    "\n",
    "This is where corpus linguistics starts to get interesting. In order to analyze a text computationally, it is useful to know its syntactic structure - what words are nouns, what are verbs, and so on? This can be done (again, imperfectly) by using *part-of-speech tagging.* NLTK includes a default part-of-speech tagger, although this probably won't be of much use to you on non-English texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "my_text = alice[305:549]\n",
    "print(pos_tag(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK part-of-speech tags (simplified tagset)\n",
    "------------------------\n",
    "\n",
    "| Tag | Meaning            | Examples                             |\n",
    "|-----|--------------------|--------------------------------------|\n",
    "| JJ  | adjective          | new, good, high, special, big, local |\n",
    "| RB  | adverb             | really, already, still, early, now   |\n",
    "| CC  | conjunction        | and, or, but, if, while, although    |\n",
    "| DT  | determiner         | the, a, some, most, every, no        |\n",
    "| EX  | existential        | there, there's                       |\n",
    "| FW  | foreign word       | dolce, ersatz, esprit, quo, maitre   |\n",
    "| MD  | modal verb         | will, can, would, may, must, should  |\n",
    "| NN  | noun               | year, home, costs, time, education   |\n",
    "| NNP | proper noun        | Alison, Africa, April, Washington    |\n",
    "| NUM | number             | twenty-four, fourth, 1991, 14:24     |\n",
    "| PRO | pronoun            | he, their, her, its, my, I, us       |\n",
    "| IN  | preposition        | on, of, at, with, by, into, under    |\n",
    "| TO  | the word to        | to                                   |\n",
    "| UH  | interjection       | ah, bang, ha, whee, hmpf, oops       |\n",
    "| VB  | verb               | is, has, get, do, make, see, run     |\n",
    "| VBD | past tense         | said, took, told, made, asked        |\n",
    "| VBG | present participle | making, going, playing, working      |\n",
    "| VN  | past participle    | given, taken, begun, sung            |\n",
    "| WRB | wh determiner      | who, which, when, what, where, how   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated tagging is pretty good, but not perfect. There are other taggers out there that handle different languages, such as the Brill tagger and the TreeTagger, but these aren't set up to run 'out of the box' and, with TreeTagger in particular, you will have to download extra software.\n",
    "\n",
    "Some of the bigger corpora in NLTK come pre-tagged; this is a useful way to __train__ a tagger that uses machine-learning methods (such as Brill), and a good way to test any new tagging method that is developed. This is also the data from which our knowledge of how language is used comes from. (At least, English and some other major Western languages.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "print(brown.tagged_words()[:25])\n",
    "print(brown.tagged_words(tagset='universal')[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even do a frequency plot of the different parts of speech in the corpus (if we have `matplotlib` installed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_word_fd = FreqDist([ w[1] for w in \n",
    "                           brown.tagged_words(tagset='universal') ])\n",
    "tagged_word_fd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named-entity recognition\n",
    "------------------------\n",
    "\n",
    "As well as the parts of speech of individual words, it is useful to be able to analyze the structure of an entire sentence. This generally involves breaking the sentence up into its component phrases, otherwise known as chunking. \n",
    "\n",
    "Not going to cover chunking here as there is no out-of-the-box chunker for NLTK! You are expected to define the grammar (or at least some approximation of the grammar), and once you have done that then it becomes possible.\n",
    "\n",
    "But one application of chunking is named-entity recognition - parsing a sentence to identify the named people, places, and organizations therein. This is more difficult than it looks, e.g. \"Yankee\", \"May\", \"North\".\n",
    "\n",
    "Here's how to do it. We will use the example sentences that were loaded in `sent1` through `sent9` to try it out. Notice the difference (in iPython only!) between printing the result and just looking at the result - if you try to show the graph for more than one sentence at a time then you'll be waiting a *long* time. So don't try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "tagged_text = pos_tag(sent2)\n",
    "ner_text = ne_chunk( tagged_text )\n",
    "print(ner_text)\n",
    "ner_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function that takes the result of `ne_chunk` (the plain-text form, not the graph form!) and spits out only the named entities that were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_named_entities( tree ):\n",
    "    try:\n",
    "        tree.label()\n",
    "    except AttributeError:\n",
    "        return\n",
    "    if( tree.label() != \"S\" ):\n",
    "        print(tree)\n",
    "    else:\n",
    "        for child in tree:\n",
    "            list_named_entities( child )\n",
    "            \n",
    "list_named_entities( ner_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it - an introductory tour of what is probably the best-available code toolkit for natural language processing. If this sort of thing interests you, then there is an entire book-length tutorial about it:\n",
    "\n",
    "http://www.nltk.org/book/\n",
    "\n",
    "Have fun!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
